{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrator Agent Tutorial\n",
    "\n",
    "This tutorial demonstrates how to create an orchestrator agent using the Atomic Agents library. The orchestrator agent will manage tasks and respond to user inputs in rhyming verse.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KennyVaneetvelde/atomic_agents/blob/main/examples/notebooks/orchestrator.ipynb#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Before proceeding with this notebook, it is highly recommended to read up on the basics of the following libraries:\n",
    "\n",
    "- **Pydantic**: A data validation and settings management library using Python type annotations. You can find more information and documentation at [Pydantic GitHub](https://github.com/pydantic/pydantic).\n",
    "- **Instructor**: A Python library that simplifies working with structured outputs from large language models (LLMs). It provides a user-friendly API to manage validation, retries, and streaming responses. More details can be found at [Instructor GitHub](https://github.com/jxnl/instructor).\n",
    "\n",
    "Understanding these libraries will help you make the most of this library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Necessary Packages\n",
    "\n",
    "First, we need to install the required packages. Run the following command to install `atomic-agents`, `openai`, and `instructor` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "%pip install atomic-agents openai instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "We will import the necessary libraries for creating the orchestrator agent. Each library serves a specific purpose:\n",
    "- `ChatMemory`: Manages the chat history.\n",
    "- `BaseChatAgent`: The base class to create a custom chatbot. Can be extended for additional functionality if needed.\n",
    "- `SystemPromptGenerator` and `SystemPromptInfo`: To define and generate system prompts.\n",
    "- `BaseTool`, `CalculatorTool`, `SearxNGSearchTool`: Tools that the orchestrator agent can use to perform tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Type, Union\n",
    "from pydantic import BaseModel, Field\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "import instructor\n",
    "import openai\n",
    "from atomic_agents.lib.components.chat_memory import ChatMemory\n",
    "from atomic_agents.agents.base_chat_agent import BaseChatAgent, BaseChatAgentResponse, BaseChatAgentConfig\n",
    "from atomic_agents.lib.components.system_prompt_generator import SystemPromptContextProviderBase, SystemPromptGenerator, SystemPromptInfo\n",
    "from atomic_agents.lib.tools.base import BaseTool\n",
    "from atomic_agents.lib.tools.calculator_tool import CalculatorTool, CalculatorToolSchema\n",
    "from atomic_agents.lib.tools.search.searx_tool import SearxNGSearchTool, SearxNGSearchToolConfig, SearxNGSearchToolSchema\n",
    "\n",
    "console = Console()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up API Keys\n",
    "\n",
    "To use the **OpenAI** and **SearxNG** APIs, you need to set up your API keys. You can either enter them directly in the code or set them as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# ENTER YOUR OPENAI API KEY BELOW, OR SET IT AS AN ENVIRONMENT VARIABLE #\n",
    "##################################################################\n",
    "API_KEY = ''\n",
    "if not API_KEY:\n",
    "    # Get the environment variable\n",
    "    API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError('API key is not set. Please set the API key as a static variable or in the environment variable OPENAI_API_KEY.')\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI(api_key=API_KEY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define System Prompt Information\n",
    "\n",
    "In this step, we will define the system prompt information including background, steps, and output instructions. This helps the orchestrator agent understand how to respond to user inputs and manage tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Definition\n",
    "\n",
    "First, we define the tools that the orchestrator agent can use. These tools will help the agent perform various tasks such as searching the web or performing calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "search_tool = SearxNGSearchTool(SearxNGSearchToolConfig(base_url=os.getenv('SEARXNG_BASE_URL'), max_results=10))\n",
    "calculator_tool = CalculatorTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the tools, we can define the ToolInfoProvider. This will format the tool information in a way that is nice to put in the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolInfoProvider(SystemPromptContextProviderBase):        \n",
    "    def get_info(self) -> str:\n",
    "        response = 'The available tools are:\\n'\n",
    "        for tool in [search_tool, calculator_tool]:\n",
    "            response += f'- {tool.tool_name}: {tool.tool_description}\\n'\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt Generator\n",
    "\n",
    "Next, we define the system prompt information including background, steps, output instructions and the ToolInfoProvider we defined above. This helps the orchestrator agent understand how to respond to user inputs and manage tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt information including background, steps, and output instructions\n",
    "system_prompt = SystemPromptInfo(\n",
    "    background=[\n",
    "        'This assistant is an orchestrator of tasks designed to be helpful and efficient.',\n",
    "    ],\n",
    "    steps=[\n",
    "        'Understand the user\\'s input and the current context.',\n",
    "        'Take a step back and think methodically and step-by-step about how to proceed by using internal reasoning.',\n",
    "        'Evaluate the available tools and decide whether to use a tool based on the current context, user input, and history.',\n",
    "        'If a tool can be used, decide which tool to use and how to use it.',\n",
    "        'If a tool can be used it must always be explicitly mentioned in the internal reasoning response.',\n",
    "        'If a tool can be used, return nothing but the tool response to the user. If no tool is needed or if the tool has finished, return a chat response.',\n",
    "        'Execute the chosen tool and provide a relevant response to the user.'\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        'Provide helpful and relevant information to assist the user.',\n",
    "        'Be efficient and effective in task orchestration.',\n",
    "        'Always answer in rhyming verse.'\n",
    "    ],\n",
    "    context_providers={\n",
    "        'tools': ToolInfoProvider(title='Available tools')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize the system prompt generator with the defined system prompt and dynamic info providers\n",
    "system_prompt_generator = SystemPromptGenerator(system_prompt)\n",
    "\n",
    "# Test out the system prompt generator\n",
    "from rich.markdown import Markdown\n",
    "from rich.console import Console\n",
    "console.print(Markdown(system_prompt_generator.generate_prompt()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Chat Memory\n",
    "\n",
    "We will initialize the chat memory to store conversation history and load an initial greeting message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemory()\n",
    "initial_memory = [\n",
    "    {'role': 'assistant', 'content': 'How do you do? What can I do for you? Tell me, pray, what is your need today?'}\n",
    "]\n",
    "memory.load(initial_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define InternalReasoningResponse\n",
    "To improve the agent's responses, we define an InternalReasoningResponse class that will be interspersed with the agent's responses. This class will help the agent reason about the next steps in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define InternalReasoningResponse class\n",
    "class InternalReasoningResponse(BaseModel):\n",
    "    observation: str = Field(..., description='What is the current state of the conversation and context? What is the user saying or asking?')\n",
    "    action_plan: List[str] = Field(..., min_length=1, description='What steps could be taken to address the current observation? Is there a tool that could be used?')\n",
    "\n",
    "    class Config:\n",
    "        title = 'InternalReasoningResponse'\n",
    "        description = 'The internal reasoning response schema for the chat agent, following the ReACT pattern.'\n",
    "        json_schema_extra = {\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "        }\n",
    "\n",
    "# Define ResponseSchema class\n",
    "class ResponseSchema(BaseModel):\n",
    "    chosen_schema: Union[BaseChatAgentResponse, SearxNGSearchToolSchema, CalculatorToolSchema] = Field(..., description='The response from the chat agent, which may include the result of using a tool if one was deemed necessary.')\n",
    "    class Config:\n",
    "        title = 'ResponseSchema'\n",
    "        description = 'The response schema for the chat agent, including potential tool usage results.'\n",
    "        json_schema_extra = {\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the necessary components, we can create the orchestrator agent.\n",
    "Personally, I like to extend the OrchestratorAgentConfig even if I don't add any additional fields. This way, I can easily add additional fields in the future if needed with minimal impact on the existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define OrchestratorAgentConfig class\n",
    "class OrchestratorAgentConfig(BaseChatAgentConfig):\n",
    "    pass\n",
    "\n",
    "# Define OrchestratorAgent class\n",
    "class OrchestratorAgent(BaseChatAgent):\n",
    "    def _pre_run(self):\n",
    "        self.memory.add_message('assistant', 'First, I will plan my steps and think about the tools at my disposal.')\n",
    "        response = self.get_response(response_model=InternalReasoningResponse)\n",
    "        self.memory.add_message('assistant', f'INTERNAL THOUGHT: I have observed \"{response.observation}\" and will take the following steps: {\", \".join(response.action_plan)}')\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can create an instance of the orchestrator agent and start interacting with it. Try asking it questions such as _\"What is 10 + log(5)?\"_ or _\"Can you find information about the history of the internet?\"_ and see how it responds.\n",
    "\n",
    "To keep this guide simple and to the point, we don't actually call the tools and do further processing, however since everything is typed you can check the type of the response and call the appropriate tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of OrchestratorAgent with the specified configuration\n",
    "agent = OrchestratorAgent(\n",
    "    config=OrchestratorAgentConfig(\n",
    "        client=instructor.from_openai(openai.OpenAI()),\n",
    "        model='gpt-3.5-turbo',\n",
    "        system_prompt_generator=system_prompt_generator,\n",
    "        memory=memory,\n",
    "        output_schema=ResponseSchema\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the initial message from the assistant\n",
    "console.print(f'Agent: {initial_memory[0][\"content\"]}')\n",
    "\n",
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    if user_input.lower() in ['/exit', '/quit']:\n",
    "        print('Exiting chat...')\n",
    "        break\n",
    "\n",
    "    response = agent.run(agent.input_schema(chat_input=user_input))\n",
    "    console.print(f'Agent: {response}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
